{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Prompt**: a \"prompt\" is the input text given to the model to generate a response. It is the question, command, or instruction that you provide to guide the model's output.\n",
    "\n",
    "## Elements of a Prompt\n",
    "\n",
    "* Instruction: the main text of the prompt that tells the model what to do.\n",
    "* Context: External information that the model should consider to generate a more relevant and accurate response.\n",
    "* Input: The question or input that the model must answer or respond to.\n",
    "* Response Format: The structure or format that the model must adhere to when generating a response. Typically format is provided as a json object or a pydantic object.\n",
    "\n",
    "source: [Prompt Engineering Guide](https://www.promptingguide.ai/introduction/elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful assistant that provides accurate and concise answers to questions based on current news articles.\n",
      "\n",
      "### Context:\n",
      "Yesterday (on 12/01/2011), the stock market crashed due to the ongoing trade war between the US and China. The Dow Jones Industrial Average dropped by 800 points, the worst single-day drop of the year. Investors are concerned about the impact of the trade war on the global economy. The US President has announced new tariffs on Chinese goods, and China has responded with retaliatory measures. The trade tensions have escalated, leading to fears of a prolonged economic downturn.\n",
      "\n",
      "### Input:\n",
      "What caused the stock market crash yesterday?\n",
      "\n",
      "### Response Format:\n",
      "{'response': '', 'date': ''}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that provides accurate and concise answers to questions based on current news articles.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response Format:\n",
    "{response_format}\n",
    "\"\"\"\n",
    "\n",
    "context = \"Yesterday (on 12/01/2011), the stock market crashed due to the ongoing trade war between the US and China. The Dow Jones Industrial Average dropped by 800 points, the worst single-day drop of the year. Investors are concerned about the impact of the trade war on the global economy. The US President has announced new tariffs on Chinese goods, and China has responded with retaliatory measures. The trade tensions have escalated, leading to fears of a prolonged economic downturn.\"\n",
    "question = \"What caused the stock market crash yesterday?\"\n",
    "\n",
    "response_format = {\n",
    "    \"response\": \"\",\n",
    "    \"date\": \"\",\n",
    "}\n",
    "\n",
    "prompt = prompt_template.format(context=context, input=question, response_format=response_format)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'response': 'The stock market crash yesterday was caused by escalating trade tensions between the US and China, including the announcement of new tariffs by the US and retaliatory measures from China, which raised concerns about the global economy.', 'date': '12/01/2011'}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Settings\n",
    "Large Language Models (LLMs) have various settings that control their output behavior. Understanding these settings can help us generate more appropriate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "This setting controls how random or predictable the model’s outputs are. A lower temperature makes the model more predictable, often picking the most likely next word. This is good for tasks like answering factual questions. A higher temperature adds randomness, which can make outputs more creative or varied—useful for things like creative writing. Basically, lower temperatures are for concise, factual responses, while higher temperatures are for creativity.\n",
    "\n",
    "**Effect:** \n",
    "- **Low Temperature:** Predictable and repetitive responses.\n",
    "- **High Temperature:** More varied and creative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp 0:  The ocean is a vast, mysterious expanse teeming with diverse life and ecosystems.\n",
      "Temp 1.5:  The ocean is a vast, fluid expanse, teeming with diverse life and mysteries.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a short description (less than 15 words) about the ocean\"\n",
    "\n",
    "def complete(prompt, temperature):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "print(\"Temp 0: \",complete(prompt, 0))\n",
    "\n",
    "print(\"Temp 1.5: \",complete(prompt, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Tokens\n",
    "This controls how many words the model generates. Setting a max length helps you avoid overly long or off-topic responses and also keeps costs down.\n",
    "\n",
    "**Effect:** \n",
    "- **Low Maximum Tokens:** Very short responses.\n",
    "- **High Maximum Tokens:** Longer, more detailed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens 10:  Photosynthesis is the biochemical process by which green plants\n",
      "Max tokens 30: Photosynthesis is a biochemical process through which green plants, algae, and certain bacteria convert light energy, usually from the sun, into chemical energy in the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"explain photosynsthesis.\"\n",
    "\n",
    "def complete(prompt, max_tokens):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "print(\"Max tokens 10: \",complete(prompt, 10))\n",
    "print(\"Max tokens 30:\",complete(prompt, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top P (Nucleus Sampling)\n",
    "**Explanation:** This is another setting that influences randomness. If Top P is low, the model only considers the most confident words for the response. If Top P is higher, model considers a wider range of words, making the responses more diverse. Generally, tweak either Temperature or Top P, not both.\n",
    "\n",
    "**Effect:** \n",
    "- **Low Top P:** Only very high-probability words are considered, resulting in more predictable responses.\n",
    "- **High Top P:** More diversity and creativity in responses by including lower-probability words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top p 0.2:  The sun dipped below the horizon, painting the sky in hues of orange, pink, and purple, whispering day’s farewell.\n",
      "Top p 0.9:  The sky blazes in fiery oranges and pinks, as the sun dips below the horizon, casting a warm, golden glow.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Describe a Sunset in less than 20 words.\"\n",
    "\n",
    "def complete(prompt, top_p):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        top_p=top_p,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "print(\"Top p 0.2: \",complete(prompt, 0.1))\n",
    "print(\"Top p 0.9: \",complete(prompt, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Penalty\n",
    "This reduces the chance of words repeating by applying a penalty based on how often they’ve already appeared. A higher frequency penalty means the model repeats words less, keeping responses varied.\n",
    "\n",
    "**Effect:** \n",
    "- **Low Frequency Penalty:** More repetitions.\n",
    "- **High Frequency Penalty:** Less repetition, encourages use of varied words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top p 0.2:  Dogs are loyal companions, known for their intelligence, friendliness, and diverse breeds, serving roles in work and love.\n",
      "Top p 0.9:  Dogs are loyal, social animals known for companionship, intelligence, and diverse breeds. They bond closely with humans.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about dogs in less than 20 words.\"\n",
    "\n",
    "def complete(prompt, frequency_penalty):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "print(\"Freq penalty -1\",complete(prompt, -1))\n",
    "print(\"Freq penalty 1.5\",complete(prompt, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence Penalty\n",
    "This also discourages repetition but treats all repeats the same, whether a word appears twice or ten times. A higher presence penalty makes the output more varied, while a lower one keeps it more focused. Use higher values for creative text and lower ones for more consistent, on-topic responses. Alter either one of Frequency Penalty or Presence Penalty, not both.\n",
    "\n",
    "**Effect:** \n",
    "- **Low Presence Penalty:** Sticks closely to the main topic.\n",
    "- **High Presence Penalty:** Introduces new and varied topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence penalty -1:  Space exploration involves the investigation of celestial bodies, using technology to gather data and understand the universe. It enhances our knowledge of planetary systems, potential extraterrestrial life, and Earth's origins. Notable missions include the Apollo moon landings, Mars rovers, and the Hubble Space Telescope, shaping humanity's view of space.\n",
      "Presence penalty 1.5:  Space exploration involves studying celestial bodies and phenomena beyond Earth, advancing our understanding of the universe. It encompasses robotic missions, crewed spaceflights, and telescopic observations, aiming to uncover cosmic origins, search for extraterrestrial life, and foster technological innovations, ultimately expanding humanity's presence in space.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Discuss space exploration in less than 50 words.\"\n",
    "\n",
    "def complete(prompt, presence_penalty):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        presence_penalty=presence_penalty,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "print(\"Presence penalty -1: \",complete(prompt, -1))\n",
    "print(\"Presence penalty 1.5: \",complete(prompt, 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect on Response When Each Setting is Decreased or Increased\n",
    "\n",
    "| Setting             | Decrease                                 | Increase                                  |\n",
    "|---------------------|------------------------------------------|-------------------------------------------|\n",
    "| **Temperature**     | More predictable and repetitive output.  | More creative, diverse, and risk-taking.  |\n",
    "| **Maximum Tokens**  | Shorter, less detailed responses.        | Longer, more detailed responses.          |\n",
    "| **Top P**           | Limited to high-probability words; safer output. | Broader word choice; more varied output.  |\n",
    "| **Frequency Penalty** | More repetition of words or phrases.   | Less repetition; more varied vocabulary.  |\n",
    "| **Presence Penalty** | Sticks closely to current topics.       | More new topics and ideas introduced.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Best Practices\n",
    "Prompt engineering tactics recommended by [OpenAI](https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
